{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75147ad",
   "metadata": {},
   "source": [
    "# Universal Bank -WE04 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b5e867f",
   "metadata": {},
   "source": [
    "1.0 Import and install python libraries we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3137ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d0c1c",
   "metadata": {},
   "source": [
    "# 2.0 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a33445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubank=pd.read_csv(\"UniversalBank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91141db1",
   "metadata": {},
   "source": [
    "# Check the missing values by summing the total na's for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c68d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubank.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86145c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4698\n",
       "1     302\n",
       "Name: CD Account, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seeing the number of 0's and 1's in the target variable\n",
    "ubank['CD Account'].value_counts()\n",
    "\n",
    "#We can clearly observe that the data have a huge imbalance in it which can effect the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90733d3b",
   "metadata": {},
   "source": [
    "# Dropping least important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd84fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubank.drop(ubank.columns[[0,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b2931",
   "metadata": {},
   "source": [
    "# Data split -train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7bb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation and training set\n",
    "train_df, test_df = train_test_split(ubank, test_size=0.3)\n",
    "\n",
    "# to reduce repetition in later code, create variables to represent the columns\n",
    "# that are our predictors and target\n",
    "target = 'CD Account'\n",
    "predictors = list(ubank.columns)\n",
    "predictors.remove(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fda425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard scaler and fit it to the training set of predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "cols_to_stdize = predictors               \n",
    "               \n",
    "# Transform the predictors of training and validation sets\n",
    "train_df[cols_to_stdize] = scaler.fit_transform(train_df[cols_to_stdize]) # train_predictors is not a numpy array\n",
    "test_df[cols_to_stdize] = scaler.transform(test_df[cols_to_stdize]) # validation_target is now a series object\n",
    "\n",
    "train_X = train_df[predictors]\n",
    "train_y = train_df[target] # train_target is now a series objecttrain_df.to_csv('airbnb_train_df.csv', index=False)\n",
    "test_X = train_df[predictors]\n",
    "test_y = test_df[target] # validation_target is now a series object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58938496",
   "metadata": {},
   "source": [
    "# Logistic Regression using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95069a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 890, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "925 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "330 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "355 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.66627907        nan 0.66627907 0.66627907 0.67082452 0.31469345\n",
      "        nan        nan 0.66627907        nan 0.25095137        nan\n",
      " 0.05940803 0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.37399577 0.                nan        nan\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907 0.05940803\n",
      "        nan 0.05940803        nan        nan 0.66627907 0.37399577\n",
      " 0.66627907        nan 0.66627907 0.66627907 0.66627907 0.66627907\n",
      " 0.05940803 0.25095137 0.66627907 0.66627907 0.25095137 0.66627907\n",
      " 0.66627907        nan        nan 0.         0.66627907        nan\n",
      " 0.66627907        nan 0.66627907 0.37399577 0.66627907 0.66627907\n",
      " 0.66627907 0.66627907 0.67082452 0.66627907 0.66627907        nan\n",
      " 0.66627907        nan 0.66627907 0.05940803 0.66627907 0.37399577\n",
      " 0.05940803 0.                nan 0.66627907 0.66627907        nan\n",
      "        nan 0.                nan        nan        nan 0.66627907\n",
      " 0.67082452 0.         0.25095137 0.         0.25095137 0.05940803\n",
      " 0.66627907        nan 0.66627907        nan        nan 0.66627907\n",
      "        nan        nan        nan 0.66627907        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.31469345 0.66627907 0.05940803\n",
      "        nan 0.66627907        nan 0.25095137        nan 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907        nan\n",
      " 0.31469345 0.66627907 0.         0.66627907        nan        nan\n",
      "        nan 0.66627907 0.37399577 0.31469345 0.         0.05940803\n",
      " 0.66627907 0.37399577 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.05940803 0.66627907 0.66627907 0.66627907 0.         0.66627907\n",
      "        nan        nan 0.66627907 0.66627907        nan        nan\n",
      " 0.05940803 0.66627907        nan        nan        nan        nan\n",
      " 0.66627907        nan 0.66627907        nan        nan        nan\n",
      " 0.31469345 0.67082452        nan 0.66627907 0.66627907        nan\n",
      "        nan        nan 0.66627907 0.37399577        nan 0.66627907\n",
      " 0.66627907 0.66627907        nan 0.66627907 0.66627907 0.66627907\n",
      "        nan        nan 0.31469345        nan 0.                nan\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907 0.\n",
      " 0.66627907 0.66627907 0.66627907 0.37399577 0.66627907        nan\n",
      " 0.66627907 0.66627907        nan 0.37399577 0.31469345        nan\n",
      "        nan 0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.05940803        nan        nan\n",
      " 0.66627907 0.25095137        nan        nan 0.37399577 0.66627907\n",
      "        nan        nan 0.66627907        nan 0.66627907        nan\n",
      " 0.25095137 0.67082452 0.66627907        nan 0.66627907 0.\n",
      " 0.67082452 0.66627907 0.66627907 0.66627907 0.67082452        nan\n",
      "        nan        nan 0.25095137        nan 0.66627907 0.66627907\n",
      "        nan 0.25095137        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.66627907 0.67082452        nan\n",
      " 0.66627907        nan        nan 0.                nan 0.31469345\n",
      " 0.66627907 0.05940803        nan 0.66627907 0.                nan\n",
      " 0.66627907 0.                nan 0.66627907        nan 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.66627907 0.67082452        nan\n",
      "        nan 0.66627907 0.         0.66627907        nan        nan\n",
      "        nan        nan        nan        nan 0.66627907 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.         0.66627907\n",
      "        nan 0.66627907        nan 0.05940803 0.05940803        nan\n",
      "        nan 0.66627907        nan        nan 0.31469345 0.66627907\n",
      " 0.37399577 0.66627907 0.66627907 0.66627907        nan        nan\n",
      "        nan 0.05940803 0.66627907        nan        nan 0.37399577\n",
      " 0.66627907 0.66627907 0.66627907 0.37399577        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.05940803        nan        nan        nan\n",
      " 0.31469345 0.66627907 0.66627907 0.66627907 0.31469345 0.05940803\n",
      " 0.25095137 0.05940803        nan        nan 0.66627907 0.66627907\n",
      "        nan        nan 0.25095137 0.25095137 0.05940803 0.66627907\n",
      " 0.66627907 0.37399577        nan 0.66627907 0.66627907        nan\n",
      " 0.66627907 0.66627907 0.         0.66627907 0.         0.66627907\n",
      "        nan 0.66627907        nan 0.31469345 0.25095137 0.\n",
      "        nan 0.25095137        nan 0.67082452        nan        nan\n",
      " 0.66627907 0.         0.66627907        nan 0.67082452 0.66627907\n",
      " 0.                nan 0.66627907        nan 0.66627907 0.\n",
      "        nan        nan        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.05940803        nan 0.37399577\n",
      " 0.37399577 0.66627907 0.66627907 0.66627907        nan        nan\n",
      "        nan 0.25095137        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.         0.66627907        nan 0.66627907 0.66627907\n",
      "        nan 0.         0.66627907        nan 0.05940803 0.66627907\n",
      " 0.05940803        nan 0.37399577        nan 0.66627907        nan\n",
      "        nan 0.05940803        nan        nan 0.         0.25095137\n",
      " 0.66627907 0.66627907 0.31469345        nan        nan 0.31469345\n",
      " 0.37399577        nan 0.66627907 0.05940803        nan 0.31469345\n",
      " 0.         0.66627907 0.66627907        nan 0.31469345        nan\n",
      " 0.66627907        nan 0.         0.66627907 0.25095137 0.66627907\n",
      " 0.05940803        nan 0.05940803 0.66627907 0.66627907 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.         0.\n",
      " 0.                nan        nan        nan        nan 0.67082452\n",
      " 0.66627907        nan        nan 0.66627907        nan 0.66627907\n",
      " 0.         0.66627907        nan 0.66627907 0.66627907 0.66627907\n",
      "        nan        nan        nan        nan 0.66627907 0.05940803\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.66664286        nan 0.66664286 0.66664286 0.66892857 0.3196039\n",
      "        nan        nan 0.66664286        nan 0.25567532        nan\n",
      " 0.05936364 0.66664286 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.42005195 0.                nan        nan\n",
      "        nan 0.66664286        nan 0.66664286 0.66664286 0.05936364\n",
      "        nan 0.05936364        nan        nan 0.66664286 0.42005195\n",
      " 0.66664286        nan 0.66664286 0.66664286 0.66664286 0.66664286\n",
      " 0.05936364 0.25567532 0.66664286 0.66664286 0.25567532 0.66664286\n",
      " 0.66664286        nan        nan 0.         0.66664286        nan\n",
      " 0.66664286        nan 0.66664286 0.42005195 0.66664286 0.66664286\n",
      " 0.66664286 0.66664286 0.66892857 0.66664286 0.66664286        nan\n",
      " 0.66664286        nan 0.66664286 0.05936364 0.66664286 0.42005195\n",
      " 0.05936364 0.                nan 0.66664286 0.66664286        nan\n",
      "        nan 0.                nan        nan        nan 0.66664286\n",
      " 0.66892857 0.         0.25567532 0.         0.25567532 0.05936364\n",
      " 0.66664286        nan 0.66664286        nan        nan 0.66664286\n",
      "        nan        nan        nan 0.66664286        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.31846104 0.66664286 0.05936364\n",
      "        nan 0.66664286        nan 0.25567532        nan 0.66664286\n",
      "        nan 0.66664286        nan 0.66664286 0.66664286        nan\n",
      " 0.3196039  0.66664286 0.         0.66664286        nan        nan\n",
      "        nan 0.66664286 0.42005195 0.3196039  0.         0.05936364\n",
      " 0.66664286 0.42005195 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.05936364 0.66664286 0.66664286 0.66664286 0.         0.66664286\n",
      "        nan        nan 0.66664286 0.66664286        nan        nan\n",
      " 0.05936364 0.66664286        nan        nan        nan        nan\n",
      " 0.66664286        nan 0.66664286        nan        nan        nan\n",
      " 0.31846104 0.66892857        nan 0.66664286 0.66664286        nan\n",
      "        nan        nan 0.66664286 0.42005195        nan 0.66664286\n",
      " 0.66664286 0.66664286        nan 0.66664286 0.66664286 0.66664286\n",
      "        nan        nan 0.3196039         nan 0.                nan\n",
      "        nan 0.66664286        nan 0.66664286 0.66664286 0.\n",
      " 0.66664286 0.66664286 0.66664286 0.42005195 0.66664286        nan\n",
      " 0.66664286 0.66664286        nan 0.42005195 0.31846104        nan\n",
      "        nan 0.66664286 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.05936364        nan        nan\n",
      " 0.66664286 0.25567532        nan        nan 0.42005195 0.66664286\n",
      "        nan        nan 0.66664286        nan 0.66664286        nan\n",
      " 0.25567532 0.66892857 0.66664286        nan 0.66664286 0.\n",
      " 0.66892857 0.66664286 0.66664286 0.66664286 0.66892857        nan\n",
      "        nan        nan 0.25567532        nan 0.66664286 0.66664286\n",
      "        nan 0.25567532        nan        nan        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.66664286 0.66892857        nan\n",
      " 0.66664286        nan        nan 0.                nan 0.31846104\n",
      " 0.66664286 0.05936364        nan 0.66664286 0.                nan\n",
      " 0.66664286 0.                nan 0.66664286        nan 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.66664286 0.66892857        nan\n",
      "        nan 0.66664286 0.         0.66664286        nan        nan\n",
      "        nan        nan        nan        nan 0.66664286 0.66664286\n",
      "        nan 0.66664286        nan 0.66664286 0.         0.66664286\n",
      "        nan 0.66664286        nan 0.05936364 0.05936364        nan\n",
      "        nan 0.66664286        nan        nan 0.3196039  0.66664286\n",
      " 0.42005195 0.66664286 0.66664286 0.66664286        nan        nan\n",
      "        nan 0.05936364 0.66664286        nan        nan 0.42005195\n",
      " 0.66664286 0.66664286 0.66664286 0.42005195        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.05936364        nan        nan        nan\n",
      " 0.31846104 0.66664286 0.66664286 0.66664286 0.31846104 0.05936364\n",
      " 0.25567532 0.05936364        nan        nan 0.66664286 0.66664286\n",
      "        nan        nan 0.25567532 0.25567532 0.05936364 0.66664286\n",
      " 0.66664286 0.42005195        nan 0.66664286 0.66664286        nan\n",
      " 0.66664286 0.66664286 0.         0.66664286 0.         0.66664286\n",
      "        nan 0.66664286        nan 0.31846104 0.25567532 0.\n",
      "        nan 0.25567532        nan 0.66892857        nan        nan\n",
      " 0.66664286 0.         0.66664286        nan 0.66892857 0.66664286\n",
      " 0.                nan 0.66664286        nan 0.66664286 0.\n",
      "        nan        nan        nan        nan        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.05936364        nan 0.42005195\n",
      " 0.42005195 0.66664286 0.66664286 0.66664286        nan        nan\n",
      "        nan 0.25567532        nan        nan        nan 0.66664286\n",
      " 0.66664286 0.         0.66664286        nan 0.66664286 0.66664286\n",
      "        nan 0.         0.66664286        nan 0.05936364 0.66664286\n",
      " 0.05936364        nan 0.42005195        nan 0.66664286        nan\n",
      "        nan 0.05936364        nan        nan 0.         0.25567532\n",
      " 0.66664286 0.66664286 0.3196039         nan        nan 0.31846104\n",
      " 0.42005195        nan 0.66664286 0.05936364        nan 0.31846104\n",
      " 0.         0.66664286 0.66664286        nan 0.31846104        nan\n",
      " 0.66664286        nan 0.         0.66664286 0.25567532 0.66664286\n",
      " 0.05936364        nan 0.05936364 0.66664286 0.66664286 0.66664286\n",
      "        nan 0.66664286        nan 0.66664286 0.         0.\n",
      " 0.                nan        nan        nan        nan 0.66892857\n",
      " 0.66664286        nan        nan 0.66664286        nan 0.66664286\n",
      " 0.         0.66664286        nan 0.66664286 0.66664286 0.66664286\n",
      "        nan        nan        nan        nan 0.66664286 0.05936364\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c327c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'C': 0.09999999999999998, 'max_iter': 490, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "4000 fits failed out of a total of 8000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0, inf]. Got -0.9 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.67082452 0.67082452 0.67082452]\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.66892857 0.66892857 0.66892857]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdad17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1500, 3500]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\732751880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mFP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mFN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \"\"\"\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    398\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1500, 3500]"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_lr={TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989158",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0f54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n",
      "The best recall score is 0.7214611872146118\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'scale', 'degree': 4, 'coef0': 6, 'C': 30.1}\n"
     ]
    }
   ],
   "source": [
    "#Random search\n",
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "SVM_R_out = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = SVM_R_out, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestSVMrandom = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3a10236",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'poly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\922073488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscore_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbest_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'poly'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbest_gamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmin_regulization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'30.1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'poly'"
     ]
    }
   ],
   "source": [
    "#grid search \n",
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['poly']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['30.1']\n",
    "best_degree = rand_search.best_params_['4']\n",
    "best_coef0=rand_search.best_params_['6']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "SVM_G_out = SVC()\n",
    "grid_search = GridSearchCV(estimator = SVM_G_out, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestSVMgrid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2625113",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_svm={TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a726f861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'performance_svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\4087344334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m performance= pd.concat([performance_svm, pd.DataFrame({'model':\"svm using Random & Grid search\", \n\u001b[0m\u001b[0;32m      2\u001b[0m                                                     \u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                     \u001b[1;34m'Precision'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                     \u001b[1;34m'Recall'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                     \u001b[1;34m'F1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'performance_svm' is not defined"
     ]
    }
   ],
   "source": [
    "performance= pd.concat([performance_svm, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56625012",
   "metadata": {},
   "source": [
    "#  Decision Trees regressor using RandomSearch and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb7cdad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\2297243613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                            return_train_score=True)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The best {score_measure} score is {rand_search.best_score_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1769\u001b[0m             ParameterSampler(\n\u001b[0;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmore_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[0;32m    311\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_without_replacement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\utils\\_random.pyx\u001b[0m in \u001b[0;36msklearn.utils._random.sample_without_replacement\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,200),  \n",
    "    'min_samples_leaf': np.arange(1,200),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['squared_error', 'poisson', 'absolute_error', 'friedman_mse'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4c0924",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'min_samples_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\689819834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscore_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmin_samples_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_samples_split'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_samples_leaf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_impurity_decrease'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'min_samples_split'"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbff7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 1.0\n",
      "... with parameters: {'min_samples_split': 38, 'min_samples_leaf': 28, 'min_impurity_decrease': 0.0071, 'max_leaf_nodes': 29, 'max_depth': 8, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "#decision tree classifier\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,42),  \n",
    "    'min_samples_leaf': np.arange(1,42),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 42), \n",
    "    'max_depth': np.arange(1,42), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc93f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "The best precision score is 0.923391812865497\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 40, 'max_leaf_nodes': 37, 'min_impurity_decrease': 0.0009, 'min_samples_leaf': 3, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(3,6),  \n",
    "    'min_samples_leaf': np.arange(3,6),\n",
    "    'min_impurity_decrease': np.arange(0.0009, 0.0012,0.0001),\n",
    "    'max_leaf_nodes': np.arange(36,40), \n",
    "    'max_depth': np.arange(39,42), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
